{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting a Sequence of Simulations using RickFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RickFlow workflow provides an interface for running simulations in OpenMM using CHARMM input files.\n",
    "\n",
    "This example (`rickflow/examples/start_simulation`) contains the following files:\n",
    "    \n",
    "- hxdwat.crd: The CHARMM coordinate file of a water-hexadecane slab system.\n",
    "- hxdwat.psf: The CHARMM psf file (connectivity) of the system.\n",
    "- top_all36_lipid.rtf and par_all36_lipid.prm: CHARMM topology and parameter files.\n",
    "\n",
    "\n",
    "- **dyn.py**: The python script that runs the simulation.\n",
    "- **sdyn.sh**: The slurm submit script.\n",
    "\n",
    "The first bunch of files define the simulation system and will vary for every system.\n",
    "The second bunch of files run the simulations on a GPU node of cluster.\n",
    "\n",
    "To start the simulations, call\n",
    "`\n",
    "rflow submit sdyn.sh\n",
    "`\n",
    "\n",
    "\n",
    "Let's take a look at the files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *dyn.py*  - The Simulation Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An executable python script that defines the simulation. First, the shebang for enabling execution via `./dyn.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of imports (note that openmm and the rflow package have to be installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import simtk.unit as u\n",
    "from simtk.openmm.app import CharmmPsfFile, CharmmCrdFile, CharmmParameterSet, PME\n",
    "from simtk.openmm.app import HBonds, Simulation, DCDReporter, StateDataReporter\n",
    "from simtk.openmm import LangevinIntegrator, MonteCarloAnisotropicBarostat\n",
    "from rflow.integrators import NoseHooverChainVelocityVerletIntegrator\n",
    "from rflow import RickFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `RickFlow` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = RickFlow(\n",
    "    toppar=[\"top_all36_lipid.rtf\", \"par_all36_lipid.prm\"],\n",
    "    psf=\"hxdwat.psf\",\n",
    "    crd=\"hxdwat.crd\",\n",
    "    box_dimensions=[50,50,53.1975], # box lenghts in Angstrom\n",
    "    gpu_id=0,\n",
    "    nonbonded_method=PME,\n",
    "    tmp_output_dir=os.path.join(\"/lscratch\", os.environ['SLURM_JOB_ID'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command does the following:\n",
    "1. It **creates an OpenMM system** (accessible as `workflow.system`) based on the files provided. \n",
    "2. It also makes sure that **CUDA** is enabled (if not, the constructor will throw an error). To skip the CUDA check, use `gpu_id=None`. \n",
    "3. It sets up the simulation on GPU 0 (note that it is usually most efficient to run jobs on single GPUs in OpenMM). `gpu_id=0` is usually the right choice. Note that the sdyn.sh script below requests only one GPU; this GPU will always have the ID `0`.\n",
    "4. It **recenters** the input coordinate so that the center of mass of all non-water atoms is at the center of the box. This  is useful, because OpenMM defines the origin (0,0,0) to be an edge of the box, while CHARMM simulations usually have the box centered around the origin.\n",
    "5. It **removes isotropic long-range corrections** from all nonbonded forces in the system. These forces are added by default, but most CHARMM parameters were optimized without long-range corrections.\n",
    "6. It sets up a particular **directory structure**: subdirectories res, trj, out for restart, trajectory, and output files, respectively. \n",
    "7. It creates a file `next.seqno`, which stores the id of the next sequence to be simulated (each sequence spans 1 ns). After a sequence is finished, the sdyn.sh script submits the next one. To stop the simulation after a given sequence, you can create a file `last.seqno`, which contains the number of the final sequence.\n",
    "8. If `tmp_output_dir` is specified, the output files will be written to a temporary directory (usually a local scratch directory on the compute node) and copied over to the working directory afterwards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Intergrator and Barostat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  ========= Define Integrator and Barostat ==========\n",
    "#\n",
    "temperature = 310.0 * u.kelvin\n",
    "# integrator (The Nose-Hoover integrator in openmm does is currently not using the right\n",
    "# number of degrees of freedom. The implementation in nosehoover.py provides a short-term fix,\n",
    "# which requires the system to be passed to the constructor)\n",
    "integrator = NoseHooverChainVelocityVerletIntegrator(\n",
    "        workflow.system, temperature, 50.0 / u.picosecond, 1.0 * u.femtosecond,\n",
    "        chain_length=3, num_mts=3, num_yoshidasuzuki=3\n",
    ")\n",
    "# integrator = LangevinIntegrator(temperature, 1.0 / u.picosecond, 1.0 * u.femtosecond)\n",
    "barostat = MonteCarloAnisotropicBarostat(\n",
    "    u.Quantity(value=np.array([0.0, 0.0, 1.0]), unit=u.atmosphere), temperature,\n",
    "    False, False, True\n",
    ")\n",
    "\n",
    "workflow.prepareSimulation(integrator, barostat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prepare simulation command creates a simulation object (`workflow.simulation`), reads the restart files, and writes out the system as a pdb (for postprocessing in VMD).\n",
    "\n",
    "Note that the `NoseHoover...` integrator is a lot slower than the `LangevinIntegrator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    workflow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sdyn.sh -- The Batch Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --time=1:00:00 --ntasks=1 --nodes=1 -p gpu\n",
    "#SBATCH --ntasks-per-node=1 --cpus-per-task=2 --gres=lscratch:250,gpu:p100:1\n",
    "\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "sleep 10\n",
    "\n",
    "# run simulation and resubmit script\n",
    "./dyn.py && rflow submit sdyn.sh && sleep 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that we do not want to use the node exclusively,\n",
    "especially when using nodes with multiple GPUs (on lobos: k40 -- 2 GPUs, pascal -- 4 GPUs).\n",
    "OpenMM does all the work on the GPU and utilizes only one GPU per simulation. \n",
    "By requesting only on GPU per job, the rest of the GPUs can be utilized by other jobs.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
